"""Script to generate quantum circuits with LLAMA 3.

It generates valid quantum programs in a given platform (e.g. qiskit, pennylane)
when given an API to use.

--platform: str platform to test, either qiskit, pennylane, or pytket
--output_folder: str path to where to store the generated py programs. (default: program_bank)
    the new programs will be stored in a subfolder with the name the data and time
    e.g. 2024_09_30__22:50__qiskit (note the end with the title of the platform)
--prompt: str. path to the prompt template (default: mono_program_generator.jinja)
--path_api_names: str. path to the JSON file that stores all the API used in the library as a list of objects.
    Each with api_name, api_description, api_signature

The output is stored in the output folder as .py files generated by the model.


# Style
- use subfunctions appropriately
- each function has at maximum 7 lines of code of content, break them to smaller functions otherwise
- always use named arguments when calling a function
    (except for standard library functions)
- keep the style consistent to pep8 (max 80 char)
- to print the logs it uses the console from Rich library
- make sure to have docstring for each subfunction and keep it brief to the point
(also avoid comments on top of the functions)
- it uses pathlib every time that paths are checked, created or composed.
- use type annotations with typing List, Dict, Any, Tuple, Optional as appropriate


# Reference API

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
```
"""
import os
import datetime
import json
import torch
import random
import lmql
import click
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from rich.console import Console
from typing import Dict

console = Console()


def setup_model(model_id: str) -> (AutoTokenizer, AutoModelForCausalLM):
    """Load and setup the LLaMA 3 model and tokenizer."""
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        quantization_config=BitsAndBytesConfig(
            # load_in_8bit=True
            load_in_4bit=True,
        )
    )
    return tokenizer, model


def load_prompt_template(prompt_path: str) -> str:
    """Load the Jinja2 template for prompts."""
    env = Environment(loader=FileSystemLoader(os.path.dirname(prompt_path)))
    template = env.get_template(os.path.basename(prompt_path))
    return template


def generate_prompt(template, api_infos: Dict[str, str],
                    platform_name: str) -> str:
    """Generate a prompt using the Jinja2 template."""
    return template.render(
        API_INFO=api_infos,
        PLATFORM_NAME=platform_name)


def generate_code(model, tokenizer, prompt: str) -> str:
    """Generate quantum code from the model using the provided prompt."""
    input_ids = tokenizer(
        prompt, return_tensors="pt").input_ids.to(
        model.device)
    terminators = [tokenizer.eos_token_id]
    outputs = model.generate(
        input_ids=input_ids,
        max_new_tokens=512,
        eos_token_id=terminators,
        # do_sample=False,
        temperature=0.1,
        do_sample=True,
    )
    response = outputs[0][input_ids.shape[-1]:]
    return tokenizer.decode(response, skip_special_tokens=True)


def save_code(
        output_folder: str, code: str, index: int) -> None:
    """Save the generated code to a file."""
    file_path = os.path.join(output_folder, f"quantum_program_{index}.py")
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(code)
    console.log(f"Saved generated code to {file_path}")


def create_output_run_folder(platform: str, output_folder: str) -> str:
    now = datetime.datetime.now()
    timestamp = now.strftime('%Y_%m_%d__%H_%M')
    folder_name = f"{timestamp}__{platform}"
    output_path = os.path.join(output_folder, folder_name)
    Path(output_path).mkdir(parents=True, exist_ok=True)
    return output_path


def save_prompt(output_folder, index, prompt):
    file_path = os.path.join(output_folder, f"quantum_program_{index}.txt")
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(prompt)
    console.log(f"Saved prompt to {file_path}")


def parse_within_tags(code: str, start_tag: str, end_tag: str) -> str:
    """Parse the code within the specified tags."""
    start_idx = code.index(start_tag) + len(start_tag)
    remaining_code = code[start_idx:]
    end_idx = remaining_code.index(end_tag)
    return remaining_code[:end_idx]


def generate_quantum_programs(
        platform: str, output_folder: str, prompt_path: str,
        path_api_names: str) -> None:
    """Generate quantum programs for all APIs in the specified folder."""
    # Initialize the model
    target_endpoint_model = lmql.model(
        "unsloth/Meta-Llama-3.1-8B-bnb-4bit",
        endpoint="localhost:8095"
    )
    prompt_template = open(prompt_path, 'r', encoding='utf-8').read()

    result = lmql.run_sync(
        prompt_template,
        platform_name=platform,
        model=target_endpoint_model,
        max_len=8098,
        repetition_penalty=1.3,
    )
    raw_output = result.prompt
    print(result)
    code = parse_within_tags(
        raw_output,
        start_tag="<|start_header_id|>assistant<|end_header_id|>```python",
        end_tag="```")
    print(code)
    return

    with open(path_api_names, 'r', encoding='utf-8') as file:
        api_list = json.load(file)
    # remove tests to avoid generation of test cases
    api_list = [d for d in api_list if not "test_" in d["api_name"]]
    random.Random(x=42).shuffle(api_list)

    output_run_path = create_output_run_folder(platform, output_folder)

    for idx, api in enumerate(api_list):
        api["file_path"] = api["file_path"].replace(
            f"platform_repos/{platform}", "")
        api_infos = api
        prompt = generate_prompt(
            template, api_infos, platform)
        save_prompt(output_run_path, idx, prompt)
        # TODO code generation here

        save_code(output_run_path, code, idx)


@click.command()
@click.option('--platform', type=str, required=True,
              help='Platform to generate quantum circuits for (qiskit, pennylane, pytket).')
@click.option('--output_folder', type=str, default='program_bank',
              help='Path to store generated Python programs.')
@click.option('--prompt', type=str, default='mono_program_generator.lmql',
              help='Path to the Jinja2 prompt template.')
@click.option('--path_api_names', type=str, required=True,
              help='Path to JSON file with API details.')
def main(platform, output_folder, prompt, path_api_names):
    """Main function to generate quantum programs."""
    console.log("Starting quantum program generation.")
    generate_quantum_programs(
        platform=platform,
        output_folder=output_folder,
        prompt_path=prompt,
        path_api_names=path_api_names
    )
    console.log("Quantum program generation completed.")


if __name__ == "__main__":
    main()
