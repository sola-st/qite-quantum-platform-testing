"""Script to generate quantum circuits with LLAMA 3.

It generates valid quantum programs in a given platform (e.g. qiskit, pennylane)
when given an API to use.

--platform: str platform to test, either qiskit, pennylane, or pytket
--output_folder: str path to where to store the generated py programs. (default: program_bank)
    the new programs will be stored in a subfolder with the name the data and time
    e.g. 2024_09_30__22:50__qiskit (note the end with the title of the platform)
--prompt: str. path to the prompt template (default: mono_program_generator.jinja)
--path_api_names: str. path to the JSON file that stores all the API used in the library as a list of objects.
    Each with api_name, api_description, api_signature

The output is stored in the output folder as .py files generated by the model.

# Implementation
- it uses click v8 library
- it has a simple main
- it uses temperature 0 for all (for reproducibility)
- log the LLM output (removing the prompt)
- run it on cuda GPU if available
- model name: meta-llama/Meta-Llama-3-8B-Instruct
    - use the appropriate format (tokenizer.apply_chat_template() to ensure the
     conversation mode is used with messages.
- quantize it using bits and bytes in 4 bits (using `BitsAndBytesConfig` object)
- the prompt template is read from file and populated with the jinja template
    the input variables that it takes are:
    - {{API_NAME}} the name of the function/api to use
    - {{API_DESCRIPTION}} the docstring of the api
    - {{PLATFORM_NAME}}
- use batch generation of size 10 to generate 10 outputs at the time.


# Style
- it uses subfunction appropriately
- always use named arguments when calling a function
    (except for standard library functions)
- keep the style consistent to pep8 (max 80 char)
- to print the logs it uses the console from Rich library
- make sure to have docstring for each subfunction and keep it brief to the point
(also avoid comments on top of the functions)
- it uses os.path every time that it is dealing with path composition
- it uses pathlib every time that paths are checked or created.

# Reference API

```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

model_id = "meta-llama/Meta-Llama-3-8B-Instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

messages = [
    {"role": "system", "content": "You are a pirate chatbot who always responds in pirate speak!"},
    {"role": "user", "content": "Who are you?"},
]

input_ids = tokenizer.apply_chat_template(
    messages,
    add_generation_prompt=True,
    return_tensors="pt"
).to(model.device)

terminators = [
    tokenizer.eos_token_id,
    tokenizer.convert_tokens_to_ids("<|eot_id|>")
]

outputs = model.generate(
    input_ids,
    max_new_tokens=256,
    eos_token_id=terminators,
    do_sample=True,
    temperature=0.6,
    top_p=0.9,
)
response = outputs[0][input_ids.shape[-1]:]
print(tokenizer.decode(response, skip_special_tokens=True))
```
"""
import os
import datetime
import json
import torch
import random
import click
from pathlib import Path
from jinja2 import Environment, FileSystemLoader
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from rich.console import Console
from typing import Dict

console = Console()


def setup_model(model_id: str) -> (AutoTokenizer, AutoModelForCausalLM):
    """Load and setup the LLaMA 3 model and tokenizer."""
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(
        model_id,
        torch_dtype=torch.bfloat16,
        device_map="auto",
        quantization_config=BitsAndBytesConfig(
            # load_in_8bit=True
            load_in_4bit=True,
        )
    )
    return tokenizer, model


def load_prompt_template(prompt_path: str) -> str:
    """Load the Jinja2 template for prompts."""
    env = Environment(loader=FileSystemLoader(os.path.dirname(prompt_path)))
    template = env.get_template(os.path.basename(prompt_path))
    return template


def generate_prompt(template, api_infos: Dict[str, str],
                    platform_name: str) -> str:
    """Generate a prompt using the Jinja2 template."""
    return template.render(
        API_INFO=api_infos,
        PLATFORM_NAME=platform_name)


def generate_code(model, tokenizer, prompt: str) -> str:
    """Generate quantum code from the model using the provided prompt."""
    input_ids = tokenizer(
        prompt, return_tensors="pt").input_ids.to(
        model.device)
    terminators = [tokenizer.eos_token_id]
    outputs = model.generate(
        input_ids=input_ids,
        max_new_tokens=512,
        eos_token_id=terminators,
        # do_sample=False,
        temperature=0.1,
        do_sample=True,
    )
    response = outputs[0][input_ids.shape[-1]:]
    return tokenizer.decode(response, skip_special_tokens=True)


def save_code(
        output_folder: str, code: str, index: int) -> None:
    """Save the generated code to a file."""
    file_path = os.path.join(output_folder, f"quantum_program_{index}.py")
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(code)
    console.log(f"Saved generated code to {file_path}")


def create_output_run_folder(platform: str, output_folder: str) -> str:
    now = datetime.datetime.now()
    timestamp = now.strftime('%Y_%m_%d__%H_%M')
    folder_name = f"{timestamp}__{platform}"
    output_path = os.path.join(output_folder, folder_name)
    Path(output_path).mkdir(parents=True, exist_ok=True)
    return output_path


def save_prompt(output_folder, index, prompt):
    file_path = os.path.join(output_folder, f"quantum_program_{index}.txt")
    with open(file_path, 'w', encoding='utf-8') as file:
        file.write(prompt)
    console.log(f"Saved prompt to {file_path}")


def generate_quantum_programs(
        platform: str, output_folder: str, prompt_path: str,
        path_api_names: str) -> None:
    """Generate quantum programs for all APIs in the specified folder."""
    model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
    tokenizer, model = setup_model(model_id)
    template = load_prompt_template(prompt_path)

    with open(path_api_names, 'r', encoding='utf-8') as file:
        api_list = json.load(file)
    # remove tests to avoid generation of test cases
    api_list = [d for d in api_list if not "test_" in d["api_name"]]
    random.Random(x=42).shuffle(api_list)

    output_run_path = create_output_run_folder(platform, output_folder)

    for idx, api in enumerate(api_list):
        api["file_path"] = api["file_path"].replace(
            f"platform_repos/{platform}", "")
        api_infos = api
        prompt = generate_prompt(
            template, api_infos, platform)
        save_prompt(output_run_path, idx, prompt)
        code = generate_code(model, tokenizer, prompt)
        save_code(output_run_path, code, idx)


@click.command()
@click.option('--platform', type=str, required=True,
              help='Platform to generate quantum circuits for (qiskit, pennylane, pytket).')
@click.option('--output_folder', type=str, default='program_bank',
              help='Path to store generated Python programs.')
@click.option('--prompt', type=str, default='mono_program_generator.jinja',
              help='Path to the Jinja2 prompt template.')
@click.option('--path_api_names', type=str, required=True,
              help='Path to JSON file with API details.')
def main(platform, output_folder, prompt, path_api_names):
    """Main function to generate quantum programs."""
    console.log("Starting quantum program generation.")
    generate_quantum_programs(
        platform=platform,
        output_folder=output_folder,
        prompt_path=prompt,
        path_api_names=path_api_names
    )
    console.log("Quantum program generation completed.")


if __name__ == "__main__":
    main()
